#!/usr/bin/env python3
# cleanup_testbed: kill all related threads launched in testbed.

from ..common import *
from .utils.exputil import *
from .utils.trace_preprocessor import *

# Get configuration settings from config.json
client_machine_idxes = JsonUtil.getValueForKeystr(Common.scriptname, "client_machine_indexes")
cloud_machine_idx = JsonUtil.getValueForKeystr(Common.scriptname, "cloud_machine_index")
physical_machines = JsonUtil.getValueForKeystr(Common.scriptname, "physical_machines")
trace_sample_opcnt = JsonUtil.getValueForKeystr(Common.scriptname, "trace_sample_opcnt")

# Check if current machine is a client machine
if Common.cur_machine_idx not in client_machine_idxes:
    LogUtil.die(Common.scriptname, "This script is only allowed to run on client machines")

# Used to hint users for keycnt and dataset size
log_filepaths = []

# Preprocess all replayed traces
replayed_workloads = ["wikitext", "wikiimage"]
for i in range(len(replayed_workloads)):
    tmp_workload = replayed_workloads[i]

    # Get log file name
    tmp_log_filepath = "{}/tmp_trace_preprocessor_for_{}.out".format(Common.output_log_dirpath, tmp_workload)
    SubprocessUtil.tryToCreateDirectory(Common.scriptname, os.path.dirname(tmp_log_filepath))
    log_filepaths.append(tmp_log_filepath)

    # Get settings for trace preprocessor
    tmp_settings = {
        "workload_name": tmp_workload
    }

    # NOTE: MUST be the same as dataset filepath in src/common/util.c
    tmp_dataset_filepath = "{}/{}.dataset.{}".format(Common.trace_dirpath, tmp_workload, trace_sample_opcnt)
    tmp_workload_filepath = "{}/{}.workload.{}".format(Common.trace_dirpath, tmp_workload, trace_sample_opcnt)

    # (1) Launch trace preprocessor (will generate dataset file)
    is_generate_dataset_file = False
    if os.path.exists(tmp_dataset_filepath) and os.path.exists(tmp_workload_filepath):
        LogUtil.prompt(Common.scriptname, "Dataset file {} and workload file {} already exist, skip trace preprocessing...".format(tmp_dataset_filepath, tmp_workload_filepath))
    else:
        LogUtil.prompt(Common.scriptname, "preprocess workload {} in current machine...".format(tmp_workload))
        tmp_trace_preprocessor = TracePreprocessor(trace_preprocessor_logfile = tmp_log_filepath, **tmp_settings)
        tmp_trace_preprocessor.run()

        is_generate_dataset_file = True

    # (2) Copy dataset file to cloud machine if not exist
    if Common.cur_machine_idx == client_machine_idxes[0]: # NOTE: ONLY copy if the current machine is the first client machine to avoid duplicate copying
        if is_generate_dataset_file:
            # Check dataset file is generated successfully
            LogUtil.prompt(Common.scriptname, "check if dataset file {} is generated successfully in current machine...".format(tmp_dataset_filepath))
            if not os.path.exists(tmp_dataset_filepath):
                LogUtil.die(Common.scriptname, "Dataset file not found: {}".format(tmp_dataset_filepath))

        if cloud_machine_idx == Common.cur_machine_idx: # NOTE: clients and cloud may co-locate in the same physical machine
            # NOTE: NO need to check and copy, as current physical machine MUST have the dataset file, which is just generated by trace preprocessor
            continue
        else: # Remote cloud machine
            # Check if cloud machine has dataset file
            # NOTE: cloud machine may already have the dataset file (e.g., trace has been preprocessed before, or cloud machine is one of client machines yet not the first client)
            LogUtil.prompt(Common.scriptname, "check if dataset file {} exists in cloud machine...".format(tmp_dataset_filepath))
            check_cloud_dataset_filepath_remote_cmd = ExpUtil.getRemoteCmd(cloud_machine_idx, "ls {}".format(tmp_dataset_filepath))
            need_copy_dataset_file = False
            check_cloud_dataset_filepath_subprocess = SubprocessUtil.runCmd(check_cloud_dataset_filepath_remote_cmd)
            if check_cloud_dataset_filepath_subprocess.returncode != 0: # cloud dataset file not found
                need_copy_dataset_file = True
            # elif SubprocessUtil.getSubprocessOutputstr(check_cloud_dataset_filepath_subprocess) == "": # cloud dataset file not found (OBSOLETE: existing empty directory could return empty string)
            #     need_copy_dataset_file = True
            else: # cloud dataset file is found
                need_copy_dataset_file = False

            # If cloud machine does NOT have the dataset file, copy it to cloud machine
            if need_copy_dataset_file:
                # Mkdir for trace dirpath if not exist
                LogUtil.prompt(Common.scriptname, "create trace directory {} if not exist in cloud machine...".format(Common.trace_dirpath))
                try_to_create_trace_dirpath_remote_cmd = ExpUtil.getRemoteCmd(cloud_machine_idx, "mkdir -p {}".format(Common.trace_dirpath))
                try_to_create_trace_dirpath_subprocess = SubprocessUtil.runCmd(try_to_create_trace_dirpath_remote_cmd)
                if try_to_create_trace_dirpath_subprocess.returncode != 0:
                    LogUtil.die(Common.scriptname, SubprocessUtil.getSubprocessErrstr(try_to_create_trace_dirpath_subprocess))

                # Copy dataset file to cloud machine
                LogUtil.prompt(Common.scriptname, "copy dataset file {} to cloud machine...".format(tmp_dataset_filepath))
                cloud_machine_public_ip = physical_machines[cloud_machine_idx]["public_ipstr"]
                copy_dataset_file_remote_cmd = "scp -i {0} {1} {2}@{3}:{1}".format(Common.sshkey_filepath, tmp_dataset_filepath, Common.username, cloud_machine_public_ip)
                copy_dataset_file_subprocess = SubprocessUtil.runCmd(copy_dataset_file_remote_cmd, is_capture_output=False) # Copy dataset file may be time-consuming
                if copy_dataset_file_subprocess.returncode != 0:
                    LogUtil.die(Common.scriptname, SubprocessUtil.getSubprocessErrstr(copy_dataset_file_subprocess))

# (3) Hint users to check keycnt and dataset size in log files, and update keycnt in config.json if necessary
# NOTE: we comment the following code as we have already updated config.json with correct keycnt, and calculate cache memory capacity in corresponding exps correctly
#LogUtil.emphasize(Common.scriptname, "Please check keycnt and dataset size in the following log files, and update keycnt in config.json accordingly if necessary:\n{}".format(log_filepaths))